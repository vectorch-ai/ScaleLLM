syntax = "proto3";

option go_package = "github.com/vectorch-ai/scalellm;scalellm";
package llm.proto;

import "common.proto";

// Next ID: 21
message CompletionRequest {
  // ID of the model to use. (required)
  // You can use the ListModels endpoint to list available models.
  string model = 1;

  // the prompt to generate completions for. (required)
  string prompt = 2;

  // TODO: the suffix that comes after a completion of inserted text. default = null
  // string suffix = 3;

  // number of tokens to generate
  // the prompt token count + max_tokens can't exceed the model's max context length.
  optional uint32 max_tokens = 4;

  // number of completions to return for each prompt. default = 1
  optional uint32 n = 7;

  // whether to stream partial completions back as they are generated. default = false
  optional bool stream = 8;

  // include the log probabilities of the chosen tokens. the maximum value is 5.
  optional uint32 logprobs = 9;

  // whether to include the original prompt in the completion response. default = true
  optional bool echo = 10;

  // temperature of the sampling, between [0, 2]. default = 1.0
  // higher value will make the ouput more random.
  optional float temperature = 5;

  // presence penalty to reduce the likelihood of generating words already in the prompt.
  // values between [-2.0, 2.0]. Positive values penalize new tokens based on their existing
  // in the prompt. default = 0.0
  optional float presence_penalty = 12;

  // frequency penalty to reduce the likelihood of generating the same word multiple times.
  // values between [0.0, 2.0]. 0.0 means no penalty. default = 0.0
  // Positive values penalize new tokens based on their existing frequency in the text.
  optional float frequency_penalty = 13;

  // repetition penalty to penalize new tokens based on their occurence in the
  // text. values > 1.0 encourage the model to use new tokens, while values
  // < 1.0 encourage the model to repeat tokens. default = 1.0
  optional float repetition_penalty = 20;

  // top_p sampling cutoff, between [0, 1.0]. default = 1.0
  optional float top_p = 6;

  // top_k sampling cutoff, default = -1 (no cutoff)
  optional int64 top_k = 19;

  // A unique identifier representing your end-user, which can help system to monitor and detect abuse.
  string user = 16;

  // whether to skip special tokens in the output. default = true
  optional bool skip_special_tokens = 14;

  // whether to ignore the end of sequence token. default = false.
  optional bool ignore_eos = 15;

  // up to 4 sequences where the API will stop generating further tokens.
  repeated string stop = 11;

  // the list of token ids where the API will stop generating further tokens.
  repeated int32 stop_token_ids = 18;

  // TODO: logit_bias
  // modify the likelihood of specified tokens appearing in the completion.
  // map<int64, float> logit_bias = 18;

  // request priority. default = DEFAULT
  optional Priority priority = 17;
}

message LogProbMap {
  map<string, float> map = 1;
}

message LogProbs {
  repeated int32 text_offset = 1 [json_name="text_offset"];

  repeated float token_logprobs = 2 [json_name="token_logprobs"];
  
  repeated string tokens = 3;
  
  repeated int32 token_ids = 4 [json_name="token_ids"];
  
  // grpc doesn't allow: repeated map<string, float> top_logprobs = 5;
  repeated LogProbMap top_logprobs = 5 [json_name="top_logprobs"];
}

message Choice {
  // the generated completion
  optional string text = 1;

  // the log probability of of output tokens.
  optional LogProbs logprobs = 2;

  // the index of the generated completion
  optional uint32 index = 3;

  // the reason of the model stoped generating tokens.
  // "stop" - the model hit a natural stop point or a provided stop sequence.
  // "length" - the maximum number of tokens specified in the request was reached.
  // "function_call" - the model called a function.
  optional string finish_reason = 4 [json_name="finish_reason"];
}

message CompletionResponse {
  // unique id for the completion request
  string id = 1;

  // the object type, which is always "text_completion".
  string object = 2;

  // the unix timestamp (in seconds) of when the completion was created.
  uint32 created = 3;

  // the model used for the completion
  string model = 4;

  // list of generated completion choices for the input prompt
  repeated Choice choices = 5;

  // usage statistics for the completion request.
  Usage usage = 6;
}

service Completion {
  // legacy API
  rpc Complete(CompletionRequest) returns (stream CompletionResponse) {}
}

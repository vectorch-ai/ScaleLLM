syntax = "proto3";

option go_package = "github.com/vectorch-ai/scalellm;scalellm";
package llm;

message Usage {
  // the number of tokens in the prompt.
  optional int32 prompt_tokens = 1 [json_name="prompt_tokens"];

  // the number of tokens in the generated completion.
  optional int32 completion_tokens = 2 [json_name="completion_tokens"];

  // the total number of tokens used in the request (prompt + completion).
  optional int32 total_tokens = 3 [json_name="total_tokens"];;  
}

enum Priority {
  DEFAULT = 0;

  HIGH = 1;

  MEDIUM = 2;

  LOW = 3;
}
